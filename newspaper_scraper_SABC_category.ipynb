{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import Source\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we wanted to download articles from SABC (which is a web site for South African broadcaster)\n",
    "# Link of the publication (SABC). include category of news\n",
    "link = \"https://www.sabcnews.com/sabcnews/category/south-africa/\"\n",
    "gamespot = newspaper.build(\"https://www.sabcnews.com/sabcnews/category/south-africa/\", memoize_articles = False) \n",
    "\n",
    "# I set memoize_articles to False, because I don't want it to cache and save the articles to memory, run after run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fresh run, everytime we run execute this script essentially\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "limit = 2000\n",
    "# limit = 5\n",
    "count = 0\n",
    "count_no_section =0\n",
    "i = 1\n",
    "while i<600:\n",
    "    gamespot = newspaper.build(link, memoize_articles = False)\n",
    "    for each_article in gamespot.articles:\n",
    "        if count > limit: # Lets have a limit, so it doesnt take too long when you're\n",
    "                break         # running the code. NOTE: You may not want to use a limit\n",
    "\n",
    "        try:\n",
    "            each_article.download()\n",
    "            each_article.parse()\n",
    "            each_article.nlp()\n",
    "        except:\n",
    "            final_df.to_csv('scraped_articles_SABC_final_interrupt_SA_pages.tsv', sep='\\t')\n",
    "            print(\"interrupted scraping..........\",count_no_section)\n",
    "            break\n",
    "        \n",
    "\n",
    "        temp_df = pd.DataFrame(columns = ['source_URL','Title',  'Text',\n",
    "                                        'Summary', 'published_date', 'Authors','Section'])\n",
    "\n",
    "        temp_df['Authors'] = each_article.authors\n",
    "        temp_df['Title'] = each_article.title\n",
    "        temp_df['Text'] = each_article.text\n",
    "        temp_df['Summary'] = each_article.summary\n",
    "        temp_df['published_date'] = each_article.publish_date\n",
    "        temp_df['source_URL'] = each_article.url\n",
    "        # temp_df['Source'] = each_article.source_url\n",
    "        try:\n",
    "            temp_df['Section'] = each_article.meta_data['article']['section']\n",
    "            print(each_article.meta_data['article']['section'])\n",
    "        except:\n",
    "            temp_df['Section'] = \"Undefined\"\n",
    "            count_no_section += 1\n",
    "\n",
    "        #Unique articles\n",
    "\n",
    "        # if temp_df in final_df.values:\n",
    "        #     print(temp_df,\"found\")\n",
    "        # else:\n",
    "        #     final_df = final_df.append(temp_df, ignore_index = True)\n",
    "        final_df = final_df.append(temp_df, ignore_index = True)    \n",
    "        # Update count\n",
    "        count +=1\n",
    "        print(\"scraped = \",count) \n",
    "\n",
    "    print(\"df size ==\",len(final_df))\n",
    "    i +=1\n",
    "    link = \"https://www.sabcnews.com/sabcnews/category/south-africa/page/\"+str(i)+\"/\"\n",
    "    final_df = final_df.drop_duplicates(subset=['source_URL'])\n",
    "    final_df.to_csv('final/articles_SABC_SA_pages_'+str(i)+'.tsv', sep='\\t')\n",
    "    print(\"done scraping..........\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here you can export this Pandas DataFrame to a csv file\n",
    "# final_df.to_csv('my_scraped_articles_SABC6.csv')\n",
    "final_df = final_df.drop_duplicates(subset=['source_URL'])\n",
    "final_df.to_csv('final/articles_SABC_final_SA_pages.tsv', sep='\\t')\n",
    "print(\"done scraping..........\",count_no_section)\n",
    "\n",
    "# csv.writer(open('scraped_articles_SABC_7.tsv', 'w+'), delimiter='\\t').writerows(csv.reader(open(\"my_scraped_articles_SABC6.csv\", encoding=\"utf8\",errors=\"ignore\")))\n",
    "# print(\"DONE.....\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
