{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sections = [\"south-africa\", \"africa\", \"world\", \"lifestyle\", \"sport\",\"sci-tech\",\"politics\",\"business\"]\n",
    "sections = [\"south-africa\", \"africa\", \"world\", \"lifestyle\", \"sport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_section = sections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 2\n",
    "limit = 100\n",
    "pages = 16\n",
    "for sec in sections:\n",
    "    current_section = sec\n",
    "    pages = 16\n",
    "    while pages<150:\n",
    "        pages +=1\n",
    "        nbc_url=\"https://www.sabcnews.com/sabcnews/category/\"+current_section+\"/page/\"+str(pages)+\"/\"\n",
    "\n",
    "        r = requests.get(nbc_url)\n",
    "\n",
    "        b = soup(r.content,'lxml')\n",
    "\n",
    "        # for news in b.findAll('h2'):\n",
    "        #     print(news.text)\n",
    "\n",
    "        links = []\n",
    "        titles = []\n",
    "        summaries = []\n",
    "\n",
    "\n",
    "        # for news in b.findAll('span',{'class':'sabc_cat_list_item_title'}):\n",
    "        for news in b.findAll('div',{'class':'sabc_cat_list_item'}):\n",
    "            links.append(news.a['href'])\n",
    "            # print(news.a.text)\n",
    "            titles.append(news.a.text)\n",
    "            # print(news.p.text)\n",
    "            summaries.append(news.p.text)\n",
    "            \n",
    "        # print(links)\n",
    "\n",
    "        author = \"\"\n",
    "        date_issued = \"\"\n",
    "        article_text = \"\"\n",
    "        i = 0\n",
    "\n",
    "\n",
    "        for link in links:\n",
    "            page = requests.get(link)\n",
    "            bsobj = soup(page.content)\n",
    "\n",
    "            date_issued = bsobj.find('span',{'class':'create'})\n",
    "            author = bsobj.find('span',{'class':'author'})\n",
    "            # print(\"date =\",date_issued.text.strip())\n",
    "            # print(\"auth =\",author.text.strip())\n",
    "            try:\n",
    "\n",
    "                author = author.text.strip()\n",
    "            except:\n",
    "                author = \"N/A\"\n",
    "            try:        \n",
    "                date_issued = date_issued.text.strip()\n",
    "            except:\n",
    "                date_issued = \"N/A\"\n",
    "            temp_text = \"\"\n",
    "            for news in bsobj.findAll('div',{'class':'post-content'}):\n",
    "                # print(news.text.strip())\n",
    "                temp_text = temp_text + news.text.strip()+\"\\n\"\n",
    "            \n",
    "            article_text = temp_text\n",
    "\n",
    "            temp_dictionary = {\n",
    "                'Authors' : author,\n",
    "                'Title' : titles[i],\n",
    "                'Text' : article_text,\n",
    "                'Summary' : summaries[i],\n",
    "                'published_date' : date_issued,\n",
    "                'source_URL':  link \n",
    "            }\n",
    "            # print(temp_dictionary)\n",
    "\n",
    "            final_df = final_df.append(temp_dictionary, ignore_index = True)\n",
    "            \n",
    "            # Update count\n",
    "            i +=1\n",
    "            print(\"scraped = \",i)\n",
    "\n",
    "\n",
    "        final_df = final_df.drop_duplicates(subset=['source_URL'])\n",
    "        # print(final_df.size)\n",
    "    final_df.to_csv('final6extension/scraped_'+current_section+'_SABC_'+str(pages)+'.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "print(\"done scraping..........\")   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
